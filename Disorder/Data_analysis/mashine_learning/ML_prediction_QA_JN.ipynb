{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  modelID       DIS    Len        FC        FE        FH  \\\n",
      "0              0  1064003  0.409797   31.0  0.290323  0.000000  0.709677   \n",
      "1              1  1064003  0.409797   31.0  0.290323  0.000000  0.709677   \n",
      "2              2  1064003  0.409797   31.0  0.290323  0.000000  0.709677   \n",
      "3              3  1064003  0.409797   31.0  0.290323  0.000000  0.709677   \n",
      "4              4  1064003  0.409797   31.0  0.290323  0.000000  0.709677   \n",
      "5              5   966001  0.485317   23.0  0.260870  0.043478  0.695652   \n",
      "6              6   966001  0.485317   23.0  0.260870  0.043478  0.695652   \n",
      "7              7   966001  0.485317   23.0  0.260870  0.043478  0.695652   \n",
      "8              8   966001  0.485317   23.0  0.260870  0.043478  0.695652   \n",
      "9              9   966001  0.485317   23.0  0.260870  0.043478  0.695652   \n",
      "10            10   549001  0.701490   35.0  0.371429  0.000000  0.628571   \n",
      "11            11   549001  0.701490   35.0  0.371429  0.000000  0.628571   \n",
      "12            12   549001  0.701490   35.0  0.371429  0.000000  0.628571   \n",
      "13            13   549001  0.701490   35.0  0.371429  0.000000  0.628571   \n",
      "14            14   549001  0.701490   35.0  0.371429  0.000000  0.628571   \n",
      "15            15  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "16           780  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "17            16  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "18           781  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "19            17  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "20           782  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "21            18  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "22           783  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "23            19  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "24           784  1841001  0.132633   51.0  0.450980  0.000000  0.549020   \n",
      "25            20   748009  0.189102   56.0  0.428571  0.553571  0.017857   \n",
      "26            21   748009  0.189102   56.0  0.428571  0.553571  0.017857   \n",
      "27            22   748009  0.189102   56.0  0.428571  0.553571  0.017857   \n",
      "28            23   748009  0.189102   56.0  0.428571  0.553571  0.017857   \n",
      "29            24   748009  0.189102   56.0  0.428571  0.553571  0.017857   \n",
      "...          ...      ...       ...    ...       ...       ...       ...   \n",
      "1115        1110  1437001  0.307548   66.0  0.212121  0.000000  0.787879   \n",
      "1116        1111  1437001  0.307548   66.0  0.212121  0.000000  0.787879   \n",
      "1117        1112  1437001  0.307548   66.0  0.212121  0.000000  0.787879   \n",
      "1118        1113  1437001  0.307548   66.0  0.212121  0.000000  0.787879   \n",
      "1119        1114  1437001  0.307548   66.0  0.212121  0.000000  0.787879   \n",
      "1120        1115   333001  0.492624  417.0  0.601918  0.203837  0.194245   \n",
      "1121        1116   333001  0.492624  417.0  0.601918  0.203837  0.194245   \n",
      "1122        1117   333001  0.492624  417.0  0.601918  0.203837  0.194245   \n",
      "1123        1118   333001  0.492624  417.0  0.601918  0.203837  0.194245   \n",
      "1124        1119   333001  0.492624  417.0  0.601918  0.203837  0.194245   \n",
      "1125        1120  1474001  0.437573  130.0  0.530769  0.453846  0.015385   \n",
      "1126        1121  1474001  0.437573  130.0  0.530769  0.453846  0.015385   \n",
      "1127        1122  1474001  0.437573  130.0  0.530769  0.453846  0.015385   \n",
      "1128        1123  1474001  0.437573  130.0  0.530769  0.453846  0.015385   \n",
      "1129        1124  1474001  0.437573  130.0  0.530769  0.453846  0.015385   \n",
      "1130        1125  1316001  0.344346  135.0  0.614815  0.207407  0.177778   \n",
      "1131        1126  1316001  0.344346  135.0  0.614815  0.207407  0.177778   \n",
      "1132        1127  1316001  0.344346  135.0  0.614815  0.207407  0.177778   \n",
      "1133        1128  1316001  0.344346  135.0  0.614815  0.207407  0.177778   \n",
      "1134        1129  1316001  0.344346  135.0  0.614815  0.207407  0.177778   \n",
      "1135        1135  2507001  0.078199  283.0  0.498233  0.469965  0.031802   \n",
      "1136        1136  2507001  0.078199  283.0  0.498233  0.469965  0.031802   \n",
      "1137        1137  2507001  0.078199  283.0  0.498233  0.469965  0.031802   \n",
      "1138        1138  2507001  0.078199  283.0  0.498233  0.469965  0.031802   \n",
      "1139        1139  2507001  0.078199  283.0  0.498233  0.469965  0.031802   \n",
      "1140        1140  2530002  0.137607  788.0  0.593909  0.209391  0.196701   \n",
      "1141        1141  2530002  0.137607  788.0  0.593909  0.209391  0.196701   \n",
      "1142        1142  2530002  0.137607  788.0  0.593909  0.209391  0.196701   \n",
      "1143        1143  2530002  0.137607  788.0  0.593909  0.209391  0.196701   \n",
      "1144        1144  2530002  0.137607  788.0  0.593909  0.209391  0.196701   \n",
      "\n",
      "             A         C         E  ...  seqlength  proq4  pcons     qmean  \\\n",
      "0     0.193548  0.000000  0.032258  ...       31.0  0.445  0.976  0.762000   \n",
      "1     0.193548  0.000000  0.032258  ...       31.0  0.445  0.975  0.795000   \n",
      "2     0.193548  0.000000  0.032258  ...       31.0  0.424  0.969  0.661000   \n",
      "3     0.193548  0.000000  0.032258  ...       31.0  0.443  0.980  0.795000   \n",
      "4     0.193548  0.000000  0.032258  ...       31.0  0.447  0.961  0.789000   \n",
      "5     0.086957  0.000000  0.000000  ...       23.0  0.174  0.664  0.813000   \n",
      "6     0.086957  0.000000  0.000000  ...       23.0  0.192  0.649  0.801000   \n",
      "7     0.086957  0.000000  0.000000  ...       23.0  0.154  0.775  0.799000   \n",
      "8     0.086957  0.000000  0.000000  ...       23.0  0.150  0.761  0.795000   \n",
      "9     0.086957  0.000000  0.000000  ...       23.0  0.150  0.776  0.793000   \n",
      "10    0.114286  0.000000  0.057143  ...       35.0  0.203  0.862  0.513000   \n",
      "11    0.114286  0.000000  0.057143  ...       35.0  0.193  0.872  0.480000   \n",
      "12    0.114286  0.000000  0.057143  ...       35.0  0.240  0.768  0.578000   \n",
      "13    0.114286  0.000000  0.057143  ...       35.0  0.227  0.878  0.446000   \n",
      "14    0.114286  0.000000  0.057143  ...       35.0  0.203  0.794  0.567000   \n",
      "15    0.058824  0.000000  0.137255  ...       51.0  0.344  0.579  0.490196   \n",
      "16    0.058824  0.000000  0.137255  ...       51.0  0.344  0.579  0.490196   \n",
      "17    0.058824  0.000000  0.137255  ...       51.0  0.350  0.583  0.470588   \n",
      "18    0.058824  0.000000  0.137255  ...       51.0  0.350  0.583  0.470588   \n",
      "19    0.058824  0.000000  0.137255  ...       51.0  0.297  0.487  0.431373   \n",
      "20    0.058824  0.000000  0.137255  ...       51.0  0.297  0.487  0.431373   \n",
      "21    0.058824  0.000000  0.137255  ...       51.0  0.295  0.591  0.470588   \n",
      "22    0.058824  0.000000  0.137255  ...       51.0  0.295  0.591  0.470588   \n",
      "23    0.058824  0.000000  0.137255  ...       51.0  0.292  0.493  0.470588   \n",
      "24    0.058824  0.000000  0.137255  ...       51.0  0.292  0.493  0.470588   \n",
      "25    0.071429  0.017857  0.071429  ...       56.0  0.632  0.974  0.680000   \n",
      "26    0.071429  0.017857  0.071429  ...       56.0  0.625  0.980  0.690000   \n",
      "27    0.071429  0.017857  0.071429  ...       56.0  0.629  0.969  0.679000   \n",
      "28    0.071429  0.017857  0.071429  ...       56.0  0.630  0.981  0.671000   \n",
      "29    0.071429  0.017857  0.071429  ...       56.0  0.623  0.981  0.681000   \n",
      "...        ...       ...       ...  ...        ...    ...    ...       ...   \n",
      "1115  0.106061  0.000000  0.136364  ...       66.0  0.302  0.974  0.432000   \n",
      "1116  0.106061  0.000000  0.136364  ...       66.0  0.284  0.978  0.414000   \n",
      "1117  0.106061  0.000000  0.136364  ...       66.0  0.306  0.973  0.427000   \n",
      "1118  0.106061  0.000000  0.136364  ...       66.0  0.330  0.963  0.389000   \n",
      "1119  0.106061  0.000000  0.136364  ...       66.0  0.302  0.958  0.421000   \n",
      "1120  0.038369  0.007194  0.129496  ...      417.0  0.516  0.715  0.566000   \n",
      "1121  0.038369  0.007194  0.129496  ...      417.0  0.516  0.715  0.558000   \n",
      "1122  0.038369  0.007194  0.129496  ...      417.0  0.510  0.709  0.557000   \n",
      "1123  0.038369  0.007194  0.129496  ...      417.0  0.497  0.710  0.562000   \n",
      "1124  0.038369  0.007194  0.129496  ...      417.0  0.493  0.700  0.551000   \n",
      "1125  0.061538  0.015385  0.123077  ...      130.0  0.502  0.956  0.384000   \n",
      "1126  0.061538  0.015385  0.123077  ...      130.0  0.495  0.937  0.379000   \n",
      "1127  0.061538  0.015385  0.123077  ...      130.0  0.492  0.958  0.394000   \n",
      "1128  0.061538  0.015385  0.123077  ...      130.0  0.502  0.958  0.397000   \n",
      "1129  0.061538  0.015385  0.123077  ...      130.0  0.495  0.955  0.379000   \n",
      "1130  0.044444  0.014815  0.088889  ...      135.0  0.539  0.981  0.359000   \n",
      "1131  0.044444  0.014815  0.088889  ...      135.0  0.519  0.977  0.368000   \n",
      "1132  0.044444  0.014815  0.088889  ...      135.0  0.521  0.977  0.358000   \n",
      "1133  0.044444  0.014815  0.088889  ...      135.0  0.520  0.977  0.354000   \n",
      "1134  0.044444  0.014815  0.088889  ...      135.0  0.514  0.979  0.367000   \n",
      "1135  0.045936  0.035336  0.031802  ...      283.0  0.505  0.602  0.413000   \n",
      "1136  0.045936  0.035336  0.031802  ...      283.0  0.488  0.605  0.414000   \n",
      "1137  0.045936  0.035336  0.031802  ...      283.0  0.505  0.592  0.408000   \n",
      "1138  0.045936  0.035336  0.031802  ...      283.0  0.509  0.558  0.384000   \n",
      "1139  0.045936  0.035336  0.031802  ...      283.0  0.496  0.597  0.395000   \n",
      "1140  0.045685  0.074873  0.067259  ...      788.0  0.382  0.406  0.628173   \n",
      "1141  0.045685  0.074873  0.067259  ...      788.0  0.458  0.452  0.607868   \n",
      "1142  0.045685  0.074873  0.067259  ...      788.0  0.446  0.463  0.645939   \n",
      "1143  0.045685  0.074873  0.067259  ...      788.0  0.442  0.442  0.619289   \n",
      "1144  0.045685  0.074873  0.067259  ...      788.0  0.448  0.413  0.629442   \n",
      "\n",
      "        dan  Ncont      Neff  contact_L  tmalign   good  \n",
      "0     0.892    1.0   7.16977   0.032258  0.40900  False  \n",
      "1     0.906    1.0   7.16977   0.032258  0.40109  False  \n",
      "2     0.748    1.0   7.16977   0.032258  0.39781  False  \n",
      "3     0.906    1.0   7.16977   0.032258  0.39547  False  \n",
      "4     0.880    1.0   7.16977   0.032258  0.38935  False  \n",
      "5     0.818    0.0   4.03670   0.000000  0.41190  False  \n",
      "6     0.824    0.0   4.03670   0.000000  0.41012  False  \n",
      "7     0.822    0.0   4.03670   0.000000  0.40850  False  \n",
      "8     0.837    0.0   4.03670   0.000000  0.40777  False  \n",
      "9     0.818    0.0   4.03670   0.000000  0.40767  False  \n",
      "10    0.802    0.0   5.25207   0.000000  0.22860  False  \n",
      "11    0.754    0.0   5.25207   0.000000  0.22859  False  \n",
      "12    0.775    0.0   5.25207   0.000000  0.22854  False  \n",
      "13    0.762    0.0   5.25207   0.000000  0.22851  False  \n",
      "14    0.674    0.0   5.25207   0.000000  0.22840  False  \n",
      "15    0.788    1.0   7.06948   0.019608  0.54293   True  \n",
      "16    0.788    1.0   7.06948   0.019608  0.54293   True  \n",
      "17    0.813    1.0   7.06948   0.019608  0.53951   True  \n",
      "18    0.813    1.0   7.06948   0.019608  0.53951   True  \n",
      "19    0.728    1.0   7.06948   0.019608  0.46461  False  \n",
      "20    0.728    1.0   7.06948   0.019608  0.46461  False  \n",
      "21    0.716    1.0   7.06948   0.019608  0.46266  False  \n",
      "22    0.716    1.0   7.06948   0.019608  0.46266  False  \n",
      "23    0.746    1.0   7.06948   0.019608  0.43291  False  \n",
      "24    0.746    1.0   7.06948   0.019608  0.43291  False  \n",
      "25    0.700  101.0  13.30550   1.803571  0.41025  False  \n",
      "26    0.706  101.0  13.30550   1.803571  0.40896  False  \n",
      "27    0.714  101.0  13.30550   1.803571  0.40741  False  \n",
      "28    0.696  101.0  13.30550   1.803571  0.40657  False  \n",
      "29    0.684  101.0  13.30550   1.803571  0.40630  False  \n",
      "...     ...    ...       ...        ...      ...    ...  \n",
      "1115  0.340   13.0   3.78784   0.196970  0.19574  False  \n",
      "1116  0.318   13.0   3.78784   0.196970  0.17367  False  \n",
      "1117  0.316   13.0   3.78784   0.196970  0.16109  False  \n",
      "1118  0.322   13.0   3.78784   0.196970  0.14815  False  \n",
      "1119  0.338   13.0   3.78784   0.196970  0.16138  False  \n",
      "1120  0.364  666.0   5.13162   1.597122  0.67097   True  \n",
      "1121  0.352  666.0   5.13162   1.597122  0.66979   True  \n",
      "1122  0.360  666.0   5.13162   1.597122  0.66663   True  \n",
      "1123  0.333  666.0   5.13162   1.597122  0.66163   True  \n",
      "1124  0.323  666.0   5.13162   1.597122  0.65779   True  \n",
      "1125  0.322  238.0   7.34117   1.830769  0.15166  False  \n",
      "1126  0.293  238.0   7.34117   1.830769  0.12044  False  \n",
      "1127  0.285  238.0   7.34117   1.830769  0.12361  False  \n",
      "1128  0.306  238.0   7.34117   1.830769  0.12130  False  \n",
      "1129  0.293  238.0   7.34117   1.830769  0.12280  False  \n",
      "1130  0.297  140.0   6.90457   1.037037  0.15939  False  \n",
      "1131  0.286  140.0   6.90457   1.037037  0.14748  False  \n",
      "1132  0.291  140.0   6.90457   1.037037  0.16865  False  \n",
      "1133  0.293  140.0   6.90457   1.037037  0.16675  False  \n",
      "1134  0.287  140.0   6.90457   1.037037  0.14051  False  \n",
      "1135  0.289  359.0   5.18033   1.268551  0.53593   True  \n",
      "1136  0.285  359.0   5.18033   1.268551  0.52932   True  \n",
      "1137  0.306  359.0   5.18033   1.268551  0.52399   True  \n",
      "1138  0.281  359.0   5.18033   1.268551  0.51223   True  \n",
      "1139  0.291  359.0   5.18033   1.268551  0.50686   True  \n",
      "1140  0.204  738.0   8.41867   0.936548  0.63446   True  \n",
      "1141  0.228  738.0   8.41867   0.936548  0.62181   True  \n",
      "1142  0.214  738.0   8.41867   0.936548  0.58649   True  \n",
      "1143  0.208  738.0   8.41867   0.936548  0.55815   True  \n",
      "1144  0.199  738.0   8.41867   0.936548  0.52942   True  \n",
      "\n",
      "[1145 rows x 40 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0.976\n",
       "1       0.975\n",
       "2       0.969\n",
       "3       0.980\n",
       "4       0.961\n",
       "5       0.664\n",
       "6       0.649\n",
       "7       0.775\n",
       "8       0.761\n",
       "9       0.776\n",
       "10      0.862\n",
       "11      0.872\n",
       "12      0.768\n",
       "13      0.878\n",
       "14      0.794\n",
       "15      0.579\n",
       "16      0.579\n",
       "17      0.583\n",
       "18      0.583\n",
       "19      0.487\n",
       "20      0.487\n",
       "21      0.591\n",
       "22      0.591\n",
       "23      0.493\n",
       "24      0.493\n",
       "25      0.974\n",
       "26      0.980\n",
       "27      0.969\n",
       "28      0.981\n",
       "29      0.981\n",
       "        ...  \n",
       "1115    0.974\n",
       "1116    0.978\n",
       "1117    0.973\n",
       "1118    0.963\n",
       "1119    0.958\n",
       "1120    0.715\n",
       "1121    0.715\n",
       "1122    0.709\n",
       "1123    0.710\n",
       "1124    0.700\n",
       "1125    0.956\n",
       "1126    0.937\n",
       "1127    0.958\n",
       "1128    0.958\n",
       "1129    0.955\n",
       "1130    0.981\n",
       "1131    0.977\n",
       "1132    0.977\n",
       "1133    0.977\n",
       "1134    0.979\n",
       "1135    0.602\n",
       "1136    0.605\n",
       "1137    0.592\n",
       "1138    0.558\n",
       "1139    0.597\n",
       "1140    0.406\n",
       "1141    0.452\n",
       "1142    0.463\n",
       "1143    0.442\n",
       "1144    0.413\n",
       "Name: pcons, Length: 1145, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scikitplot as skplt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "from yellowbrick.model_selection import RFECV\n",
    "from scipy.stats import pearsonr\n",
    "#df = pd.read_csv(\"Newer_new/final_withTemplates_0.8.csv\", sep=',')\n",
    "df = pd.read_csv(\"Newer_new/Statistics/Statistic_d5.csv\", sep=',')\n",
    "dfT = pd.read_csv(\"Newer_new/Statistics/final_withTemplates_JZ2.csv\", sep=',')\n",
    "df= df.drop(['tmalign','good'],axis = 1)\n",
    "df = pd.merge(df, dfT, on=['modelID', 'Nmodel'])\n",
    "\n",
    "df[\"good\"] = (df['tmalign']>0.5)\n",
    "dff= df.drop([\"Unnamed: 0\"],axis = 1)\n",
    "print(df)\n",
    "dff.to_csv(\"Newer_new/Statistics/Statistic_d6.csv\", sep=',')\n",
    "\n",
    "\n",
    "#df['tmalign'] = df['tmalign2']\n",
    "\n",
    "#df= df.drop(['tmalign2'],axis = 1)\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(df['tmalign'])\n",
    "labelsC=  np.array(df['class'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "#df= df.drop('modelID', axis = 1)\n",
    "cor=df.corr(method = \"pearson\")\n",
    "cor.to_csv('correlation.csv')\n",
    "\n",
    "\n",
    "\n",
    "dft= df.drop(['tmalign','class','Nmodel',\"Unnamed: 0\",\"seqlength\",\"FE\",\"A\",\"G\",\"K\",\"C\",\"E\",\"D\",\"F\",\"I\",\"H\",\"M\",\"L\",\"N\",\"Q\",\"S\",\"R\",\"T\",\"W\",\"V\",\"Y\",\"good\",\"RMSD\",\"Ncont\"],axis = 1)\n",
    "\n",
    "#dft= df.drop(['tmalign','class','Nmodel',\"proq4\",\"Neff\",\"DIS\",\"FC\",\"FE\",\"FH\",\"A\",\"C\",\"E\",\"D\",\"F\",\"I\",\"H\",\"M\",\"L\",\"N\",\"Q\",\"P\",\"S\",\"R\",\"T\",\"W\",\"V\",\"Y\"],axis = 1)\n",
    "#dft= df.drop(['Nmodel',\"tmalign\",\"class\",\"Neff\",\"DIS\",\"FE\",\"FH\",\"A\",\"C\",\"E\",\"D\",\"G\",\"F\",\"I\",\"H\",\"K\",\"M\",\"L\",\"N\",\"Q\",\"P\",\"S\",\"R\",\"T\",\"W\",\"V\",\"Y\"],axis = 1)\n",
    "\n",
    "df = np.array(df)\n",
    "#dft = np.array(dft)\n",
    "\n",
    "pd.to_numeric(dft.pcons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      modelID       DIS    Len        FC        FH         P  proq4  pcons  \\\n",
      "0     1064003  0.409797   31.0  0.290323  0.709677  0.000000  0.445  0.976   \n",
      "1     1064003  0.409797   31.0  0.290323  0.709677  0.000000  0.445  0.975   \n",
      "2     1064003  0.409797   31.0  0.290323  0.709677  0.000000  0.424  0.969   \n",
      "3     1064003  0.409797   31.0  0.290323  0.709677  0.000000  0.443  0.980   \n",
      "4     1064003  0.409797   31.0  0.290323  0.709677  0.000000  0.447  0.961   \n",
      "5      966001  0.485317   23.0  0.260870  0.695652  0.000000  0.174  0.664   \n",
      "6      966001  0.485317   23.0  0.260870  0.695652  0.000000  0.192  0.649   \n",
      "7      966001  0.485317   23.0  0.260870  0.695652  0.000000  0.154  0.775   \n",
      "8      966001  0.485317   23.0  0.260870  0.695652  0.000000  0.150  0.761   \n",
      "9      966001  0.485317   23.0  0.260870  0.695652  0.000000  0.150  0.776   \n",
      "10     549001  0.701490   35.0  0.371429  0.628571  0.028571  0.203  0.862   \n",
      "11     549001  0.701490   35.0  0.371429  0.628571  0.028571  0.193  0.872   \n",
      "12     549001  0.701490   35.0  0.371429  0.628571  0.028571  0.240  0.768   \n",
      "13     549001  0.701490   35.0  0.371429  0.628571  0.028571  0.227  0.878   \n",
      "14     549001  0.701490   35.0  0.371429  0.628571  0.028571  0.203  0.794   \n",
      "15    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.344  0.579   \n",
      "16    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.344  0.579   \n",
      "17    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.350  0.583   \n",
      "18    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.350  0.583   \n",
      "19    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.297  0.487   \n",
      "20    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.297  0.487   \n",
      "21    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.295  0.591   \n",
      "22    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.295  0.591   \n",
      "23    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.292  0.493   \n",
      "24    1841001  0.132633   51.0  0.450980  0.549020  0.058824  0.292  0.493   \n",
      "25     748009  0.189102   56.0  0.428571  0.017857  0.035714  0.632  0.974   \n",
      "26     748009  0.189102   56.0  0.428571  0.017857  0.035714  0.625  0.980   \n",
      "27     748009  0.189102   56.0  0.428571  0.017857  0.035714  0.629  0.969   \n",
      "28     748009  0.189102   56.0  0.428571  0.017857  0.035714  0.630  0.981   \n",
      "29     748009  0.189102   56.0  0.428571  0.017857  0.035714  0.623  0.981   \n",
      "...       ...       ...    ...       ...       ...       ...    ...    ...   \n",
      "1115  1437001  0.307548   66.0  0.212121  0.787879  0.030303  0.302  0.974   \n",
      "1116  1437001  0.307548   66.0  0.212121  0.787879  0.030303  0.284  0.978   \n",
      "1117  1437001  0.307548   66.0  0.212121  0.787879  0.030303  0.306  0.973   \n",
      "1118  1437001  0.307548   66.0  0.212121  0.787879  0.030303  0.330  0.963   \n",
      "1119  1437001  0.307548   66.0  0.212121  0.787879  0.030303  0.302  0.958   \n",
      "1120   333001  0.492624  417.0  0.601918  0.194245  0.064748  0.516  0.715   \n",
      "1121   333001  0.492624  417.0  0.601918  0.194245  0.064748  0.516  0.715   \n",
      "1122   333001  0.492624  417.0  0.601918  0.194245  0.064748  0.510  0.709   \n",
      "1123   333001  0.492624  417.0  0.601918  0.194245  0.064748  0.497  0.710   \n",
      "1124   333001  0.492624  417.0  0.601918  0.194245  0.064748  0.493  0.700   \n",
      "1125  1474001  0.437573  130.0  0.530769  0.015385  0.053846  0.502  0.956   \n",
      "1126  1474001  0.437573  130.0  0.530769  0.015385  0.053846  0.495  0.937   \n",
      "1127  1474001  0.437573  130.0  0.530769  0.015385  0.053846  0.492  0.958   \n",
      "1128  1474001  0.437573  130.0  0.530769  0.015385  0.053846  0.502  0.958   \n",
      "1129  1474001  0.437573  130.0  0.530769  0.015385  0.053846  0.495  0.955   \n",
      "1130  1316001  0.344346  135.0  0.614815  0.177778  0.051852  0.539  0.981   \n",
      "1131  1316001  0.344346  135.0  0.614815  0.177778  0.051852  0.519  0.977   \n",
      "1132  1316001  0.344346  135.0  0.614815  0.177778  0.051852  0.521  0.977   \n",
      "1133  1316001  0.344346  135.0  0.614815  0.177778  0.051852  0.520  0.977   \n",
      "1134  1316001  0.344346  135.0  0.614815  0.177778  0.051852  0.514  0.979   \n",
      "1135  2507001  0.078199  283.0  0.498233  0.031802  0.045936  0.505  0.602   \n",
      "1136  2507001  0.078199  283.0  0.498233  0.031802  0.045936  0.488  0.605   \n",
      "1137  2507001  0.078199  283.0  0.498233  0.031802  0.045936  0.505  0.592   \n",
      "1138  2507001  0.078199  283.0  0.498233  0.031802  0.045936  0.509  0.558   \n",
      "1139  2507001  0.078199  283.0  0.498233  0.031802  0.045936  0.496  0.597   \n",
      "1140  2530002  0.137607  788.0  0.593909  0.196701  0.046954  0.382  0.406   \n",
      "1141  2530002  0.137607  788.0  0.593909  0.196701  0.046954  0.458  0.452   \n",
      "1142  2530002  0.137607  788.0  0.593909  0.196701  0.046954  0.446  0.463   \n",
      "1143  2530002  0.137607  788.0  0.593909  0.196701  0.046954  0.442  0.442   \n",
      "1144  2530002  0.137607  788.0  0.593909  0.196701  0.046954  0.448  0.413   \n",
      "\n",
      "         qmean    dan      Neff  contact_L  \n",
      "0     0.762000  0.892   7.16977   0.032258  \n",
      "1     0.795000  0.906   7.16977   0.032258  \n",
      "2     0.661000  0.748   7.16977   0.032258  \n",
      "3     0.795000  0.906   7.16977   0.032258  \n",
      "4     0.789000  0.880   7.16977   0.032258  \n",
      "5     0.813000  0.818   4.03670   0.000000  \n",
      "6     0.801000  0.824   4.03670   0.000000  \n",
      "7     0.799000  0.822   4.03670   0.000000  \n",
      "8     0.795000  0.837   4.03670   0.000000  \n",
      "9     0.793000  0.818   4.03670   0.000000  \n",
      "10    0.513000  0.802   5.25207   0.000000  \n",
      "11    0.480000  0.754   5.25207   0.000000  \n",
      "12    0.578000  0.775   5.25207   0.000000  \n",
      "13    0.446000  0.762   5.25207   0.000000  \n",
      "14    0.567000  0.674   5.25207   0.000000  \n",
      "15    0.490196  0.788   7.06948   0.019608  \n",
      "16    0.490196  0.788   7.06948   0.019608  \n",
      "17    0.470588  0.813   7.06948   0.019608  \n",
      "18    0.470588  0.813   7.06948   0.019608  \n",
      "19    0.431373  0.728   7.06948   0.019608  \n",
      "20    0.431373  0.728   7.06948   0.019608  \n",
      "21    0.470588  0.716   7.06948   0.019608  \n",
      "22    0.470588  0.716   7.06948   0.019608  \n",
      "23    0.470588  0.746   7.06948   0.019608  \n",
      "24    0.470588  0.746   7.06948   0.019608  \n",
      "25    0.680000  0.700  13.30550   1.803571  \n",
      "26    0.690000  0.706  13.30550   1.803571  \n",
      "27    0.679000  0.714  13.30550   1.803571  \n",
      "28    0.671000  0.696  13.30550   1.803571  \n",
      "29    0.681000  0.684  13.30550   1.803571  \n",
      "...        ...    ...       ...        ...  \n",
      "1115  0.432000  0.340   3.78784   0.196970  \n",
      "1116  0.414000  0.318   3.78784   0.196970  \n",
      "1117  0.427000  0.316   3.78784   0.196970  \n",
      "1118  0.389000  0.322   3.78784   0.196970  \n",
      "1119  0.421000  0.338   3.78784   0.196970  \n",
      "1120  0.566000  0.364   5.13162   1.597122  \n",
      "1121  0.558000  0.352   5.13162   1.597122  \n",
      "1122  0.557000  0.360   5.13162   1.597122  \n",
      "1123  0.562000  0.333   5.13162   1.597122  \n",
      "1124  0.551000  0.323   5.13162   1.597122  \n",
      "1125  0.384000  0.322   7.34117   1.830769  \n",
      "1126  0.379000  0.293   7.34117   1.830769  \n",
      "1127  0.394000  0.285   7.34117   1.830769  \n",
      "1128  0.397000  0.306   7.34117   1.830769  \n",
      "1129  0.379000  0.293   7.34117   1.830769  \n",
      "1130  0.359000  0.297   6.90457   1.037037  \n",
      "1131  0.368000  0.286   6.90457   1.037037  \n",
      "1132  0.358000  0.291   6.90457   1.037037  \n",
      "1133  0.354000  0.293   6.90457   1.037037  \n",
      "1134  0.367000  0.287   6.90457   1.037037  \n",
      "1135  0.413000  0.289   5.18033   1.268551  \n",
      "1136  0.414000  0.285   5.18033   1.268551  \n",
      "1137  0.408000  0.306   5.18033   1.268551  \n",
      "1138  0.384000  0.281   5.18033   1.268551  \n",
      "1139  0.395000  0.291   5.18033   1.268551  \n",
      "1140  0.628173  0.204   8.41867   0.936548  \n",
      "1141  0.607868  0.228   8.41867   0.936548  \n",
      "1142  0.645939  0.214   8.41867   0.936548  \n",
      "1143  0.619289  0.208   8.41867   0.936548  \n",
      "1144  0.629442  0.199   8.41867   0.936548  \n",
      "\n",
      "[1145 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dft)\n",
    "dft.to_csv(\"training_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=223, shuffle=False)\n",
    "kf.get_n_splits(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelID        int64\n",
      "DIS          float64\n",
      "Len          float64\n",
      "FC           float64\n",
      "FH           float64\n",
      "P            float64\n",
      "proq4        float64\n",
      "pcons        float64\n",
      "qmean        float64\n",
      "dan          float64\n",
      "Neff         float64\n",
      "contact_L    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#train_inds, test_inds = next(GroupShuffleSplit(test_size=.33, n_splits=2, random_state = 7).split(dft, groups=dfo['modelID']))\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "#train_inds, test_inds = next(gs.split(dft, labels, groups=dft['modelID']))\n",
    "groups=dft['modelID']\n",
    "print(dft.dtypes)\n",
    "#print(groups.shape)\n",
    "#print(groups)\n",
    "#print(train_inds)\n",
    "#test_inds=test_inds[4::5]\n",
    "#print(train['pcons'])\n",
    "#pd.to_numeric(train.pcons)\n",
    "#pd.to_numeric(test.pcons)\n",
    "#print(train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=223, random_state=None, shuffle=False)\n",
      "0.742325585861745\n",
      "0.5855727753943443\n",
      "0.5491683927446989\n",
      "0.7772802755477982\n",
      "0.5842301885536173\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 200, random_state = 42)\n",
    "print(kf)\n",
    "\n",
    "#rf.fit(train, labels[train_inds]);\n",
    "#predictions = rf.predict(test)\n",
    "#Pred_TM =[predictions, labels[test_inds]]\n",
    "\n",
    "\n",
    "n_fold = 1\n",
    "for train_inds, test_inds in gs.split(dft, labels, groups=dft['modelID']):\n",
    "    dft2= dft.drop('modelID',axis = 1)\n",
    "    train, test = dft2.iloc[train_inds], dft2.iloc[test_inds]\n",
    "   \n",
    "    rf.fit(train, labels[train_inds]);\n",
    "    predictions = rf.predict(test)\n",
    "    errors = abs(predictions - labels[test_inds])\n",
    "    #print('Mean Absolute Error:', round(np.mean(errors), 5), 'TM point.')\n",
    "    mape = 100 * (errors / labels[test_inds])\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    #MAE = mean_squared_error(labels[test_index], predictions)\n",
    "    #print('MAE',MAE,)\n",
    "    #print(round(accuracy, 2))  #print('Accuracy:', round(accuracy, 2), '%.')\n",
    "    \n",
    "    Pred_TM =[predictions, labels[test_inds]]\n",
    "    \n",
    "    corr, _ = pearsonr(predictions, labels[test_inds])\n",
    "    print( corr)\n",
    "    \n",
    "    #for v in zip(*Pred_TM):\n",
    "    #    print(*v)\n",
    "    n_fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7135193902735417\n",
      "0.6114979189046016\n",
      "0.4955072439174766\n",
      "0.6798831267801404\n",
      "0.6249792972801761\n"
     ]
    }
   ],
   "source": [
    "for train_inds, test_inds in gs.split(dft, labels, groups=dft['modelID']):\n",
    "    dft2= dft.drop('modelID',axis = 1)\n",
    "    train, test = dft2.iloc[train_inds], dft2.iloc[test_inds]\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(train, labels[train_inds])\n",
    "    Pred_TM_lr = regr.predict(test)\n",
    "    corr, _ = pearsonr(Pred_TM_lr, labels[test_inds])\n",
    "    print( corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.6691869229319767\n",
      "0.6468288406400972\n",
      "0.6638211732110726\n",
      "0.6791427422333897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from pandas import DataFrame\n",
    "\n",
    "PPV =[]\n",
    "for train_inds, test_inds in gs.split(dft, labelsC, groups=dft['modelID']):\n",
    "    dft2= dft.drop('modelID',axis = 1)\n",
    "    train, test = dft2.iloc[train_inds], dft2.iloc[test_inds]\n",
    "    RF = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    RF.fit(train, labelsC[train_inds])\n",
    "    Pred_TM_RF = RF.predict(test)\n",
    "\n",
    "    #print(classification_report(Pred_TM_RF, labelsC[test_inds]))\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    predicted_proba = RF.predict_proba(test)\n",
    "    predicted = (predicted_proba [:,1] >= threshold).astype('int')\n",
    "    \n",
    "    accuracy1 = accuracy_score(labelsC[test_inds], Pred_TM_RF)\n",
    "    accuracy2= accuracy_score(labelsC[test_inds], predicted)\n",
    "    #PPV1= precision_score(labelsC[test_inds], Pred_TM_RF, average='macro')\n",
    "    PPV2= precision_score(predicted,labelsC[test_inds], average='macro')\n",
    "    #print(dft['modelID'][test_inds])\n",
    "    #print(Pred_TM_RF)\n",
    "    #print(labelsC[test_inds])\n",
    "    PPV.append(PPV2)\n",
    "    train_data = [Pred_TM_RF, labelsC[test_inds], predicted_proba[:,1]]\n",
    "    df6 = DataFrame(train_data).transpose()\n",
    "    df6.columns =['Prediction','Label','probability']\n",
    "    df6.to_csv(\"Training_result.csv\",mode='a') \n",
    "\n",
    "    \n",
    "    #PPV = np.array(PPV)\n",
    "\n",
    "    print(np.average(PPV))    \n",
    "    #print(PPV2)\n",
    "\n",
    "    \n",
    "    #or row in zip(predicted, Pred_TM_RF, predicted_proba[:,1]) :\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.86      0.83       140\n",
      "         1.0       0.73      0.65      0.69        85\n",
      "\n",
      "    accuracy                           0.78       225\n",
      "   macro avg       0.77      0.75      0.76       225\n",
      "weighted avg       0.77      0.78      0.77       225\n",
      "\n",
      "0.7666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.81      0.81       173\n",
      "         1.0       0.46      0.45      0.46        62\n",
      "\n",
      "    accuracy                           0.71       235\n",
      "   macro avg       0.63      0.63      0.63       235\n",
      "weighted avg       0.71      0.71      0.71       235\n",
      "\n",
      "0.6318070472960241\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.71      0.68       130\n",
      "         1.0       0.57      0.50      0.53       100\n",
      "\n",
      "    accuracy                           0.62       230\n",
      "   macro avg       0.61      0.60      0.60       230\n",
      "weighted avg       0.61      0.62      0.61       230\n",
      "\n",
      "0.6080345710627402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.87      0.80       128\n",
      "         1.0       0.80      0.63      0.70       107\n",
      "\n",
      "    accuracy                           0.76       235\n",
      "   macro avg       0.77      0.75      0.75       235\n",
      "weighted avg       0.76      0.76      0.75       235\n",
      "\n",
      "0.766359192683696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.72      0.80       169\n",
      "         1.0       0.52      0.77      0.62        66\n",
      "\n",
      "    accuracy                           0.74       235\n",
      "   macro avg       0.71      0.75      0.71       235\n",
      "weighted avg       0.79      0.74      0.75       235\n",
      "\n",
      "0.7054595560852078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from pandas import DataFrame\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "kf.get_n_splits(dft)\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "#train_inds, test_inds = next(gs.split(dft, labels, groups=dft['modelID']))\n",
    "groups=dft['modelID']\n",
    "\n",
    "\n",
    "\n",
    "PPV =[]\n",
    "for train_inds, test_inds in gs.split(dft, labelsC, groups=dft['modelID']):\n",
    "    dft8= dft.drop('modelID',axis = 1)\n",
    "    train, test = dft8.iloc[train_inds], dft8.iloc[test_inds]\n",
    "    RF3 = RandomForestClassifier(n_estimators=50, max_features=0.4, min_samples_leaf=35, random_state=24)\n",
    "    RF3.fit(train, labelsC[train_inds])\n",
    "    Pred_TM_RF3 = RF3.predict(test)\n",
    "\n",
    "    print(classification_report(Pred_TM_RF3, labelsC[test_inds]))\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    predicted_proba = RF3.predict_proba(test)\n",
    "    predicted = (predicted_proba [:,1] >= threshold).astype('int')\n",
    "    \n",
    "    accuracy1 = accuracy_score(labelsC[test_inds], Pred_TM_RF3)\n",
    "    accuracy2= accuracy_score(labelsC[test_inds], predicted)\n",
    "    #PPV1= precision_score(labelsC[test_inds], Pred_TM_RF, average='macro')\n",
    "    PPV2= precision_score(predicted,labelsC[test_inds], average='macro')\n",
    "    #print(dft['modelID'][test_inds])\n",
    "    #print(Pred_TM_RF)\n",
    "    #print(labelsC[test_inds])\n",
    "    PPV.append(PPV2)\n",
    "    train_data = [test_inds, Pred_TM_RF3, labelsC[test_inds], predicted_proba[:,1]]\n",
    "    df9 = DataFrame(train_data).transpose()\n",
    "    df9.columns =[\"test_ids\",'Prediction','Label','probability']\n",
    "    df9.to_csv(\"Training_result2.csv\",mode='a') \n",
    "    print(PPV2) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature: pcons (0.228900)\n",
      "2. feature: proq4 (0.148528)\n",
      "3. feature: contact_L (0.127323)\n",
      "4. feature: qmean (0.103354)\n",
      "5. feature: FH (0.096171)\n",
      "6. feature: Neff (0.083085)\n",
      "7. feature: FC (0.059764)\n",
      "8. feature: P (0.049318)\n",
      "9. feature: Len (0.045567)\n",
      "10. feature: dan (0.033577)\n",
      "11. feature: DIS (0.024414)\n"
     ]
    }
   ],
   "source": [
    "importances = RF3.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "feature_list = [\"DIS\",\"Len\",\"FC\",\"FH\",\"P\",\"proq4\",\"pcons\",\"qmean\",\"dan\",\"Neff\",\"contact_L\"] #names of features.\n",
    "ff = np.array(feature_list)\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(dft2.shape[1]):\n",
    "        print(\"%d. feature: %s (%f)\" % (f + 1, ff[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dff =  pd.read_csv(\"features.csv\")\n",
    "\n",
    "\n",
    "ax = sns.barplot(x=\"Feature\", y=\"Importance\", data=dff)\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Features\")\n",
    "fig = ax.get_figure()\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      modelID  Nmodel    C  seqlength        FC\n",
      "0       15006       1   46         76  0.605263\n",
      "1       15006       2   46         76  0.605263\n",
      "2       15006       3   46         76  0.605263\n",
      "3       15006       4   46         76  0.605263\n",
      "4       15006       5   46         76  0.605263\n",
      "5      138010       1   59         77  0.766234\n",
      "6      138010       2   59         77  0.766234\n",
      "7      138010       3   59         77  0.766234\n",
      "8      138010       4   59         77  0.766234\n",
      "9      138010       5   59         77  0.766234\n",
      "10     797001       1   52         77  0.675325\n",
      "11     797001       2   52         77  0.675325\n",
      "12     797001       3   52         77  0.675325\n",
      "13     797001       4   52         77  0.675325\n",
      "14     797001       5   52         77  0.675325\n",
      "15     931002       1   28         34  0.823529\n",
      "16     931002       2   28         34  0.823529\n",
      "17     931002       3   28         34  0.823529\n",
      "18     931002       4   28         34  0.823529\n",
      "19     931002       5   28         34  0.823529\n",
      "20    2487001       1  125        288  0.434028\n",
      "21    2487001       2  125        288  0.434028\n",
      "22    2487001       3  125        288  0.434028\n",
      "23    2487001       4  125        288  0.434028\n",
      "24    2487001       5  125        288  0.434028\n",
      "25    2487002       1   94        206  0.456311\n",
      "26    2487002       2   94        206  0.456311\n",
      "27    2487002       3   94        206  0.456311\n",
      "28    2487002       4   94        206  0.456311\n",
      "29    2487002       5   94        206  0.456311\n",
      "...       ...     ...  ...        ...       ...\n",
      "1085   128001       1   41        251  0.163347\n",
      "1086   128001       2   41        251  0.163347\n",
      "1087   128001       3   41        251  0.163347\n",
      "1088   128001       4   41        251  0.163347\n",
      "1089   128001       5   41        251  0.163347\n",
      "1090   870001       1  106        203  0.522167\n",
      "1091   870001       2  106        203  0.522167\n",
      "1092   870001       3  106        203  0.522167\n",
      "1093   870001       4  106        203  0.522167\n",
      "1094   870001       5  106        203  0.522167\n",
      "1095  1438001       1   18         67  0.268657\n",
      "1096  1438001       2   18         67  0.268657\n",
      "1097  1438001       3   18         67  0.268657\n",
      "1098  1438001       4   18         67  0.268657\n",
      "1099  1438001       5   18         67  0.268657\n",
      "1100   262006       1   27         51  0.529412\n",
      "1101   262006       2   27         51  0.529412\n",
      "1102   262006       3   27         51  0.529412\n",
      "1103   262006       4   27         51  0.529412\n",
      "1104   262006       5   27         51  0.529412\n",
      "1105  1147001       1   41         81  0.506173\n",
      "1106  1147001       2   41         81  0.506173\n",
      "1107  1147001       3   41         81  0.506173\n",
      "1108  1147001       4   41         81  0.506173\n",
      "1109  1147001       5   41         81  0.506173\n",
      "1110  1467001       1   18         80  0.225000\n",
      "1111  1467001       2   18         80  0.225000\n",
      "1112  1467001       3   18         80  0.225000\n",
      "1113  1467001       4   18         80  0.225000\n",
      "1114  1467001       5   18         80  0.225000\n",
      "\n",
      "[1115 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudio/.local/lib/python3.5/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "dfnT = pd.read_csv(\"Newer_new/final_noTemplates.csv\", sep=' ')\n",
    "dfT = pd.read_csv(\"Newer_new/final_withTemplates.csv\", sep=' ')\n",
    "\n",
    "SS = pd.read_csv(\"Newer_new/SS_fractions_template5\", sep=',')\n",
    "total = dfT.append([dfnT])\n",
    "print(SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T1 = pd.read_csv(\"Newer_new/Template_new.csv\", sep=',')\n",
    "T2 = pd.read_csv(\"Newer_new/ID_order.txt\", sep=',')\n",
    "#dfT = pd.concat([T1, T2], axis=1)\n",
    "data3 = T1.merge(T2, how='outer')\n",
    "\n",
    "#data3['Diff'] = np.where( data3['PDB1'] == data3['PDB2'] , '1', '0')\n",
    "\n",
    "data3.to_csv('Outer')\n",
    "###print(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pcons  proq4  qmeanDisCo  seqlen    dan        FC     Neff  contact_L  \\\n",
      "0     0.996  0.496       0.518    77.0  0.609  0.571429  7.74456   0.454545   \n",
      "1     0.996  0.487       0.523    77.0  0.615  0.571429  7.74456   0.454545   \n",
      "2     0.995  0.497       0.515    77.0  0.618  0.571429  7.74456   0.454545   \n",
      "3     0.995  0.496       0.518    77.0  0.646  0.571429  7.74456   0.454545   \n",
      "4     0.993  0.482       0.522    77.0  0.620  0.571429  7.74456   0.454545   \n",
      "5     0.995  0.509       0.660    41.0  0.688  0.341463  6.86236   0.975610   \n",
      "6     0.994  0.511       0.727    41.0  0.704  0.341463  6.86236   0.975610   \n",
      "7     0.994  0.498       0.658    41.0  0.707  0.341463  6.86236   0.975610   \n",
      "8     0.994  0.483       0.750    41.0  0.705  0.341463  6.86236   0.975610   \n",
      "9     0.987  0.493       0.730    41.0  0.698  0.341463  6.86236   0.975610   \n",
      "10    0.994  0.369       0.371   131.0  0.560  0.503817  4.34869   0.274809   \n",
      "11    0.994  0.383       0.399   131.0  0.526  0.503817  4.34869   0.274809   \n",
      "12    0.993  0.370       0.380   131.0  0.531  0.503817  4.34869   0.274809   \n",
      "13    0.993  0.356       0.406   131.0  0.537  0.503817  4.34869   0.274809   \n",
      "14    0.992  0.407       0.389   131.0  0.538  0.503817  4.34869   0.274809   \n",
      "15    0.993  0.550       0.678    41.0  0.719  0.414634  2.78398   1.146341   \n",
      "16    0.992  0.599       0.685    41.0  0.706  0.414634  2.78398   1.146341   \n",
      "17    0.990  0.527       0.673    41.0  0.713  0.414634  2.78398   1.146341   \n",
      "18    0.990  0.547       0.687    41.0  0.707  0.414634  2.78398   1.146341   \n",
      "19    0.989  0.572       0.696    41.0  0.677  0.414634  2.78398   1.146341   \n",
      "20    0.992  0.463       0.744   143.0  0.698  0.041958  7.60296   0.000000   \n",
      "21    0.990  0.441       0.632   143.0  0.654  0.041958  7.60296   0.000000   \n",
      "22    0.988  0.436       0.766   143.0  0.574  0.041958  7.60296   0.000000   \n",
      "23    0.987  0.423       0.489   143.0  0.566  0.041958  7.60296   0.000000   \n",
      "24    0.985  0.485       0.666   143.0  0.640  0.041958  7.60296   0.000000   \n",
      "25    0.990  0.303       0.392   103.0  0.484  0.601942  4.83688   0.087379   \n",
      "26    0.989  0.333       0.391   103.0  0.471  0.601942  4.83688   0.087379   \n",
      "27    0.988  0.300       0.402   103.0  0.495  0.601942  4.83688   0.087379   \n",
      "28    0.988  0.314       0.410   103.0  0.532  0.601942  4.83688   0.087379   \n",
      "29    0.986  0.331       0.420   103.0  0.472  0.601942  4.83688   0.087379   \n",
      "...     ...    ...         ...     ...    ...       ...      ...        ...   \n",
      "3174  0.000  0.269       0.343   206.0  0.420  0.592233  5.62672   0.014563   \n",
      "3175  0.000  0.290       0.332   206.0  0.424  0.592233  5.62672   0.014563   \n",
      "3176  0.000  0.276       0.333   206.0  0.416  0.592233  5.62672   0.014563   \n",
      "3177  0.000  0.292       0.320   206.0  0.432  0.592233  5.62672   0.014563   \n",
      "3178  0.000  0.305       0.316   206.0  0.437  0.592233  5.62672   0.014563   \n",
      "3179  0.000  0.285       0.164   526.0  0.447  0.904943  5.57097   0.195817   \n",
      "3180  0.000  0.306       0.173   526.0  0.433  0.904943  5.57097   0.195817   \n",
      "3181  0.000  0.273       0.160   526.0  0.368  0.904943  5.57097   0.195817   \n",
      "3182  0.000  0.327       0.182   526.0  0.397  0.904943  5.57097   0.195817   \n",
      "3183  0.000  0.279       0.167   526.0  0.480  0.904943  5.57097   0.195817   \n",
      "3184  0.000  0.364       0.233   114.0  0.382  0.675439  5.56416   0.105263   \n",
      "3185  0.000  0.425       0.226   114.0  0.428  0.675439  5.56416   0.105263   \n",
      "3186  0.000  0.344       0.215   114.0  0.303  0.675439  5.56416   0.105263   \n",
      "3187  0.000  0.376       0.209   114.0  0.396  0.675439  5.56416   0.105263   \n",
      "3188  0.000  0.398       0.232   114.0  0.383  0.675439  5.56416   0.105263   \n",
      "3189  0.000  0.231       0.181   310.0  0.272  0.448387  3.21980   0.048387   \n",
      "3190  0.000  0.270       0.159   310.0  0.304  0.448387  3.21980   0.048387   \n",
      "3191  0.000  0.267       0.164   310.0  0.299  0.448387  3.21980   0.048387   \n",
      "3192  0.000  0.306       0.159   310.0  0.268  0.448387  3.21980   0.048387   \n",
      "3193  0.000  0.261       0.140   310.0  0.284  0.448387  3.21980   0.048387   \n",
      "3194  0.000  0.264       0.295   108.0  0.288  0.666667  3.08853   0.074074   \n",
      "3195  0.000  0.262       0.314   108.0  0.297  0.666667  3.08853   0.074074   \n",
      "3196  0.000  0.253       0.297   108.0  0.295  0.666667  3.08853   0.074074   \n",
      "3197  0.000  0.226       0.338   108.0  0.296  0.666667  3.08853   0.074074   \n",
      "3198  0.000  0.266       0.289   108.0  0.290  0.666667  3.08853   0.074074   \n",
      "3199  0.000  0.342       0.267   172.0  0.315  0.674419  3.79833   0.261628   \n",
      "3200  0.000  0.364       0.279   172.0  0.328  0.674419  3.79833   0.261628   \n",
      "3201  0.000  0.348       0.266   172.0  0.376  0.674419  3.79833   0.261628   \n",
      "3202  0.000  0.375       0.256   172.0  0.362  0.674419  3.79833   0.261628   \n",
      "3203  0.000  0.357       0.263   172.0  0.376  0.674419  3.79833   0.261628   \n",
      "\n",
      "           DIS        FH         P  \n",
      "0     0.491374  0.415584  0.038961  \n",
      "1     0.491374  0.415584  0.038961  \n",
      "2     0.491374  0.415584  0.038961  \n",
      "3     0.491374  0.415584  0.038961  \n",
      "4     0.491374  0.415584  0.038961  \n",
      "5     0.475729  0.390244  0.048780  \n",
      "6     0.475729  0.390244  0.048780  \n",
      "7     0.475729  0.390244  0.048780  \n",
      "8     0.475729  0.390244  0.048780  \n",
      "9     0.475729  0.390244  0.048780  \n",
      "10    0.462959  0.450382  0.053435  \n",
      "11    0.462959  0.450382  0.053435  \n",
      "12    0.462959  0.450382  0.053435  \n",
      "13    0.462959  0.450382  0.053435  \n",
      "14    0.462959  0.450382  0.053435  \n",
      "15    0.464394  0.243902  0.000000  \n",
      "16    0.464394  0.243902  0.000000  \n",
      "17    0.464394  0.243902  0.000000  \n",
      "18    0.464394  0.243902  0.000000  \n",
      "19    0.464394  0.243902  0.000000  \n",
      "20    0.765051  0.958042  0.000000  \n",
      "21    0.765051  0.958042  0.000000  \n",
      "22    0.765051  0.958042  0.000000  \n",
      "23    0.765051  0.958042  0.000000  \n",
      "24    0.765051  0.958042  0.000000  \n",
      "25    0.535560  0.398058  0.048544  \n",
      "26    0.535560  0.398058  0.048544  \n",
      "27    0.535560  0.398058  0.048544  \n",
      "28    0.535560  0.398058  0.048544  \n",
      "29    0.535560  0.398058  0.048544  \n",
      "...        ...       ...       ...  \n",
      "3174  0.671540  0.393204  0.116505  \n",
      "3175  0.671540  0.393204  0.116505  \n",
      "3176  0.671540  0.393204  0.116505  \n",
      "3177  0.671540  0.393204  0.116505  \n",
      "3178  0.671540  0.393204  0.116505  \n",
      "3179  0.749053  0.049430  0.051331  \n",
      "3180  0.749053  0.049430  0.051331  \n",
      "3181  0.749053  0.049430  0.051331  \n",
      "3182  0.749053  0.049430  0.051331  \n",
      "3183  0.749053  0.049430  0.051331  \n",
      "3184  0.748445  0.184211  0.043860  \n",
      "3185  0.748445  0.184211  0.043860  \n",
      "3186  0.748445  0.184211  0.043860  \n",
      "3187  0.748445  0.184211  0.043860  \n",
      "3188  0.748445  0.184211  0.043860  \n",
      "3189  0.389380  0.541935  0.032258  \n",
      "3190  0.389380  0.541935  0.032258  \n",
      "3191  0.389380  0.541935  0.032258  \n",
      "3192  0.389380  0.541935  0.032258  \n",
      "3193  0.389380  0.541935  0.032258  \n",
      "3194  0.261090  0.212963  0.083333  \n",
      "3195  0.261090  0.212963  0.083333  \n",
      "3196  0.261090  0.212963  0.083333  \n",
      "3197  0.261090  0.212963  0.083333  \n",
      "3198  0.261090  0.212963  0.083333  \n",
      "3199  0.663247  0.063953  0.075581  \n",
      "3200  0.663247  0.063953  0.075581  \n",
      "3201  0.663247  0.063953  0.075581  \n",
      "3202  0.663247  0.063953  0.075581  \n",
      "3203  0.663247  0.063953  0.075581  \n",
      "\n",
      "[3174 rows x 11 columns]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "######Prediction######\n",
    "data = pd.read_csv(\"Newer_new/Predict_set.csv\", sep=',')\n",
    "SS = pd.read_csv(\"Newer_new/data_SS_TOT.csv\", sep=',')\n",
    "dfAA = pd.read_csv(\"Newer_new/Data_all_features.csv\", sep=',')\n",
    "data_prova= pd.read_csv(\"training_set.csv\", index_col=False, sep=',')\n",
    "#dataA = data.merge(T2, how='outer')\n",
    "df8 = pd.read_csv(\"Newer_new/Statistics/ID_numcontacts_8.csv\")\n",
    "dfn = pd.read_csv(\"Newer_new/Statistics/Neff.csv\")\n",
    "\n",
    "\n",
    "data2 = data.merge(SS, how='outer')\n",
    "data28 = data2.merge(df8, how='outer').dropna()\n",
    "data28n = data28.merge(dfn, how='outer').dropna()\n",
    "data28n[\"contact_L\"] = data28n['Ncont']/data28n['seqlen']\n",
    "data28nA = data28n.merge(dfAA, how='outer').dropna()\n",
    "#print(data28nA)\n",
    "#data4.to_csv('Data2')\n",
    "#data28nA = data28nA.drop(['modelID','Nmodel'],axis = 1)\n",
    "data28nA.to_csv('Prediction_set.csv')\n",
    "\n",
    "data4 = data28nA.drop([\"modelID\",\"Nmodel\",\"FE\",\"A\",\"G\",\"K\",\"C\",\"E\",\"D\",\"F\",\"I\",\"H\",\"M\",\"L\",\"N\",\"Q\",\"S\",\"R\",\"T\",\"W\",\"V\",\"Y\",\"Ncont\"],axis = 1)\n",
    "print(data4)\n",
    "\n",
    "\n",
    "#print(list(data_prova2.shape))\n",
    "\n",
    "\n",
    "Pred = RF3.predict(data4)\n",
    "list1 = Pred.tolist()\n",
    "print(list1)\n",
    "\n",
    "#good = []\n",
    "#good.append([index for index,ele in enumerate(list1) if ele==1])\n",
    "#print([data4.ix[good]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

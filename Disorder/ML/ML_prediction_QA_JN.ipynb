{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from scipy import stats\n",
    "from yellowbrick.model_selection import RFECV\n",
    "from scipy.stats import pearsonr\n",
    "df = pd.read_csv(\"Data_all_features.0.9_3.0.tot.csv\", sep=',',)\n",
    "#dft = pd.read_csv(\"TM_Qmean_unknow\", header = 0)\n",
    "#print(df['violation'])\n",
    "#pcons = df['Pcons']\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(df['tmalign'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "#df= df.drop('modelID', axis = 1)\n",
    "cor=df.corr(method = \"pearson\")\n",
    "cor.to_csv('correlation.csv')\n",
    "dft= df.drop(['tmalign','Nmodel',\"DIS\",\"FC\",\"FE\",\"FH\",\"A\",\"C\",\"F\",\"I\",\"H\",\"L\",\"N\",\"P\",\"S\",\"Y\"],axis = 1)\n",
    "dft['index'] = dft.index\n",
    "dft_NoDAN= df.drop(['tmalign', 'modelID','Nmodel','dan'],axis = 1)\n",
    "dft_NoDANLEN= df.drop(['tmalign', 'modelID','Nmodel','dan','seqlength'],axis = 1)\n",
    "dfo = df\n",
    "dfto=dft\n",
    "df = np.array(df)\n",
    "#dft = np.array(dft)\n",
    "\n",
    "dft_NoDAN= np.array(dft_NoDAN)\n",
    "dft_NoDANLEN=np.array(dft_NoDANLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      modelID  pcons  proq4    dan  seqlength      Neff         E         D  \\\n",
      "0     1825001  0.282  0.176  0.369         48   1.00000  0.000000  0.062500   \n",
      "1     1825001  0.265  0.154  0.456         48   1.00000  0.000000  0.062500   \n",
      "2     1825001  0.318  0.174  0.516         48   1.00000  0.000000  0.062500   \n",
      "3     1825001   0.32  0.162  0.515         48   1.00000  0.000000  0.062500   \n",
      "4     1825001  0.266  0.175  0.448         48   1.00000  0.000000  0.062500   \n",
      "5     1027001  0.577  0.136  0.448         32   1.00000  0.093750  0.093750   \n",
      "6     1027001  0.888  0.127  0.809         32   1.00000  0.093750  0.093750   \n",
      "7     1027001  0.888  0.127  0.804         32   1.00000  0.093750  0.093750   \n",
      "8     1027001  0.885  0.133  0.684         32   1.00000  0.093750  0.093750   \n",
      "9     1027001  0.872  0.131  0.862         32   1.00000  0.093750  0.093750   \n",
      "10    2042001  0.533  0.206  0.419         54   1.00000  0.111111  0.037037   \n",
      "11    2042001  0.575  0.220  0.471         54   1.00000  0.111111  0.037037   \n",
      "12    2042001  0.611  0.212  0.439         54   1.00000  0.111111  0.037037   \n",
      "13    2042001  0.449  0.188  0.429         54   1.00000  0.111111  0.037037   \n",
      "14    2042001  0.421  0.199  0.471         54   1.00000  0.111111  0.037037   \n",
      "15    2334001  0.787  0.168  0.450         53   1.00000  0.056604  0.037736   \n",
      "16    2334001  0.807  0.163  0.432         53   1.00000  0.056604  0.037736   \n",
      "17    2334001  0.769  0.162  0.520         53   1.00000  0.056604  0.037736   \n",
      "18    2334001  0.754  0.168  0.493         53   1.00000  0.056604  0.037736   \n",
      "19    2334001  0.666  0.160  0.457         53   1.00000  0.056604  0.037736   \n",
      "20    1144001  0.589  0.216  0.430        100   1.07176  0.040000  0.010000   \n",
      "21    1144001  0.618  0.201  0.460        100   1.07176  0.040000  0.010000   \n",
      "22    1144001  0.507  0.227  0.438        100   1.07176  0.040000  0.010000   \n",
      "23    1144001  0.571  0.189  0.446        100   1.07176  0.040000  0.010000   \n",
      "24    1144001  0.639  0.200  0.440        100   1.07176  0.040000  0.010000   \n",
      "25    1479001  0.448  0.142  0.385         35   1.16666  0.057143  0.057143   \n",
      "26    1479001   0.45  0.192  0.300         35   1.16666  0.057143  0.057143   \n",
      "27    1479001  0.459  0.137  0.393         35   1.16666  0.057143  0.057143   \n",
      "28    1479001  0.442  0.137  0.383         35   1.16666  0.057143  0.057143   \n",
      "29    1479001  0.394  0.135  0.360         35   1.16666  0.057143  0.057143   \n",
      "...       ...    ...    ...    ...        ...       ...       ...       ...   \n",
      "1765   752001  0.977  0.659  0.620         71  11.43210  0.070423  0.070423   \n",
      "1766   752001  0.971  0.676  0.706         71  11.43210  0.070423  0.070423   \n",
      "1767   752001   0.98  0.669  0.644         71  11.43210  0.070423  0.070423   \n",
      "1768   752001  0.965  0.654  0.704         71  11.43210  0.070423  0.070423   \n",
      "1769   752001  0.975  0.670  0.647         71  11.43210  0.070423  0.070423   \n",
      "1770   840004  0.964  0.369  0.544         35  11.81540  0.057143  0.085714   \n",
      "1771   840004  0.955  0.344  0.514         35  11.81540  0.057143  0.085714   \n",
      "1772   840004  0.966  0.359  0.540         35  11.81540  0.057143  0.085714   \n",
      "1773   840004  0.969  0.414  0.531         35  11.81540  0.057143  0.085714   \n",
      "1774   840004  0.964  0.386  0.550         35  11.81540  0.057143  0.085714   \n",
      "1775   787001  0.957  0.603  0.463        124  11.85400  0.032258  0.048387   \n",
      "1776   787001  0.961  0.621  0.484        124  11.85400  0.032258  0.048387   \n",
      "1777   787001  0.957  0.613  0.469        124  11.85400  0.032258  0.048387   \n",
      "1778   787001  0.955  0.614  0.453        124  11.85400  0.032258  0.048387   \n",
      "1779   787001  0.961  0.593  0.457        124  11.85400  0.032258  0.048387   \n",
      "1780   249001  0.985  0.663  0.560         59  11.88760  0.152542  0.169492   \n",
      "1781   249001  0.986  0.669  0.615         59  11.88760  0.152542  0.169492   \n",
      "1782   249001  0.988  0.677  0.576         59  11.88760  0.152542  0.169492   \n",
      "1783   249001  0.988  0.676  0.592         59  11.88760  0.152542  0.169492   \n",
      "1784   249001  0.987  0.661  0.586         59  11.88760  0.152542  0.169492   \n",
      "1785   621001  0.917  0.598  0.546        113  12.49190  0.070796  0.088496   \n",
      "1786   621001  0.923  0.606  0.538        113  12.49190  0.070796  0.088496   \n",
      "1787   621001  0.917  0.584  0.518        113  12.49190  0.070796  0.088496   \n",
      "1788   621001  0.903  0.585  0.492        113  12.49190  0.070796  0.088496   \n",
      "1789   621001  0.925  0.611  0.546        113  12.49190  0.070796  0.088496   \n",
      "1790   748009  0.974  0.632  0.700         56  13.30550  0.071429  0.017857   \n",
      "1791   748009   0.98  0.625  0.706         56  13.30550  0.071429  0.017857   \n",
      "1792   748009  0.981  0.630  0.696         56  13.30550  0.071429  0.017857   \n",
      "1793   748009  0.969  0.629  0.714         56  13.30550  0.071429  0.017857   \n",
      "1794   748009  0.981  0.623  0.684         56  13.30550  0.071429  0.017857   \n",
      "\n",
      "             G         K         M         Q         R         T         W  \\\n",
      "0     0.083333  0.041667  0.000000  0.020833  0.062500  0.083333  0.000000   \n",
      "1     0.083333  0.041667  0.000000  0.020833  0.062500  0.083333  0.000000   \n",
      "2     0.083333  0.041667  0.000000  0.020833  0.062500  0.083333  0.000000   \n",
      "3     0.083333  0.041667  0.000000  0.020833  0.062500  0.083333  0.000000   \n",
      "4     0.083333  0.041667  0.000000  0.020833  0.062500  0.083333  0.000000   \n",
      "5     0.031250  0.125000  0.000000  0.062500  0.156250  0.031250  0.000000   \n",
      "6     0.031250  0.125000  0.000000  0.062500  0.156250  0.031250  0.000000   \n",
      "7     0.031250  0.125000  0.000000  0.062500  0.156250  0.031250  0.000000   \n",
      "8     0.031250  0.125000  0.000000  0.062500  0.156250  0.031250  0.000000   \n",
      "9     0.031250  0.125000  0.000000  0.062500  0.156250  0.031250  0.000000   \n",
      "10    0.037037  0.000000  0.018519  0.074074  0.074074  0.074074  0.000000   \n",
      "11    0.037037  0.000000  0.018519  0.074074  0.074074  0.074074  0.000000   \n",
      "12    0.037037  0.000000  0.018519  0.074074  0.074074  0.074074  0.000000   \n",
      "13    0.037037  0.000000  0.018519  0.074074  0.074074  0.074074  0.000000   \n",
      "14    0.037037  0.000000  0.018519  0.074074  0.074074  0.074074  0.000000   \n",
      "15    0.075472  0.094340  0.037736  0.094340  0.018868  0.056604  0.018868   \n",
      "16    0.075472  0.094340  0.037736  0.094340  0.018868  0.056604  0.018868   \n",
      "17    0.075472  0.094340  0.037736  0.094340  0.018868  0.056604  0.018868   \n",
      "18    0.075472  0.094340  0.037736  0.094340  0.018868  0.056604  0.018868   \n",
      "19    0.075472  0.094340  0.037736  0.094340  0.018868  0.056604  0.018868   \n",
      "20    0.070000  0.020000  0.040000  0.060000  0.050000  0.070000  0.000000   \n",
      "21    0.070000  0.020000  0.040000  0.060000  0.050000  0.070000  0.000000   \n",
      "22    0.070000  0.020000  0.040000  0.060000  0.050000  0.070000  0.000000   \n",
      "23    0.070000  0.020000  0.040000  0.060000  0.050000  0.070000  0.000000   \n",
      "24    0.070000  0.020000  0.040000  0.060000  0.050000  0.070000  0.000000   \n",
      "25    0.028571  0.000000  0.057143  0.028571  0.085714  0.000000  0.000000   \n",
      "26    0.028571  0.000000  0.057143  0.028571  0.085714  0.000000  0.000000   \n",
      "27    0.028571  0.000000  0.057143  0.028571  0.085714  0.000000  0.000000   \n",
      "28    0.028571  0.000000  0.057143  0.028571  0.085714  0.000000  0.000000   \n",
      "29    0.028571  0.000000  0.057143  0.028571  0.085714  0.000000  0.000000   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1765  0.084507  0.056338  0.056338  0.070423  0.000000  0.112676  0.000000   \n",
      "1766  0.084507  0.056338  0.056338  0.070423  0.000000  0.112676  0.000000   \n",
      "1767  0.084507  0.056338  0.056338  0.070423  0.000000  0.112676  0.000000   \n",
      "1768  0.084507  0.056338  0.056338  0.070423  0.000000  0.112676  0.000000   \n",
      "1769  0.084507  0.056338  0.056338  0.070423  0.000000  0.112676  0.000000   \n",
      "1770  0.085714  0.114286  0.028571  0.028571  0.028571  0.000000  0.028571   \n",
      "1771  0.085714  0.114286  0.028571  0.028571  0.028571  0.000000  0.028571   \n",
      "1772  0.085714  0.114286  0.028571  0.028571  0.028571  0.000000  0.028571   \n",
      "1773  0.085714  0.114286  0.028571  0.028571  0.028571  0.000000  0.028571   \n",
      "1774  0.085714  0.114286  0.028571  0.028571  0.028571  0.000000  0.028571   \n",
      "1775  0.072581  0.056452  0.000000  0.048387  0.016129  0.088710  0.016129   \n",
      "1776  0.072581  0.056452  0.000000  0.048387  0.016129  0.088710  0.016129   \n",
      "1777  0.072581  0.056452  0.000000  0.048387  0.016129  0.088710  0.016129   \n",
      "1778  0.072581  0.056452  0.000000  0.048387  0.016129  0.088710  0.016129   \n",
      "1779  0.072581  0.056452  0.000000  0.048387  0.016129  0.088710  0.016129   \n",
      "1780  0.084746  0.084746  0.067797  0.016949  0.016949  0.050847  0.000000   \n",
      "1781  0.084746  0.084746  0.067797  0.016949  0.016949  0.050847  0.000000   \n",
      "1782  0.084746  0.084746  0.067797  0.016949  0.016949  0.050847  0.000000   \n",
      "1783  0.084746  0.084746  0.067797  0.016949  0.016949  0.050847  0.000000   \n",
      "1784  0.084746  0.084746  0.067797  0.016949  0.016949  0.050847  0.000000   \n",
      "1785  0.061947  0.035398  0.000000  0.044248  0.061947  0.053097  0.008850   \n",
      "1786  0.061947  0.035398  0.000000  0.044248  0.061947  0.053097  0.008850   \n",
      "1787  0.061947  0.035398  0.000000  0.044248  0.061947  0.053097  0.008850   \n",
      "1788  0.061947  0.035398  0.000000  0.044248  0.061947  0.053097  0.008850   \n",
      "1789  0.061947  0.035398  0.000000  0.044248  0.061947  0.053097  0.008850   \n",
      "1790  0.089286  0.089286  0.000000  0.035714  0.071429  0.053571  0.017857   \n",
      "1791  0.089286  0.089286  0.000000  0.035714  0.071429  0.053571  0.017857   \n",
      "1792  0.089286  0.089286  0.000000  0.035714  0.071429  0.053571  0.017857   \n",
      "1793  0.089286  0.089286  0.000000  0.035714  0.071429  0.053571  0.017857   \n",
      "1794  0.089286  0.089286  0.000000  0.035714  0.071429  0.053571  0.017857   \n",
      "\n",
      "             V  index  \n",
      "0     0.062500      0  \n",
      "1     0.062500      1  \n",
      "2     0.062500      2  \n",
      "3     0.062500      3  \n",
      "4     0.062500      4  \n",
      "5     0.000000      5  \n",
      "6     0.000000      6  \n",
      "7     0.000000      7  \n",
      "8     0.000000      8  \n",
      "9     0.000000      9  \n",
      "10    0.055556     10  \n",
      "11    0.055556     11  \n",
      "12    0.055556     12  \n",
      "13    0.055556     13  \n",
      "14    0.055556     14  \n",
      "15    0.037736     15  \n",
      "16    0.037736     16  \n",
      "17    0.037736     17  \n",
      "18    0.037736     18  \n",
      "19    0.037736     19  \n",
      "20    0.070000     20  \n",
      "21    0.070000     21  \n",
      "22    0.070000     22  \n",
      "23    0.070000     23  \n",
      "24    0.070000     24  \n",
      "25    0.028571     25  \n",
      "26    0.028571     26  \n",
      "27    0.028571     27  \n",
      "28    0.028571     28  \n",
      "29    0.028571     29  \n",
      "...        ...    ...  \n",
      "1765  0.084507   1765  \n",
      "1766  0.084507   1766  \n",
      "1767  0.084507   1767  \n",
      "1768  0.084507   1768  \n",
      "1769  0.084507   1769  \n",
      "1770  0.114286   1770  \n",
      "1771  0.114286   1771  \n",
      "1772  0.114286   1772  \n",
      "1773  0.114286   1773  \n",
      "1774  0.114286   1774  \n",
      "1775  0.072581   1775  \n",
      "1776  0.072581   1776  \n",
      "1777  0.072581   1777  \n",
      "1778  0.072581   1778  \n",
      "1779  0.072581   1779  \n",
      "1780  0.016949   1780  \n",
      "1781  0.016949   1781  \n",
      "1782  0.016949   1782  \n",
      "1783  0.016949   1783  \n",
      "1784  0.016949   1784  \n",
      "1785  0.026549   1785  \n",
      "1786  0.026549   1786  \n",
      "1787  0.026549   1787  \n",
      "1788  0.026549   1788  \n",
      "1789  0.026549   1789  \n",
      "1790  0.125000   1790  \n",
      "1791  0.125000   1791  \n",
      "1792  0.125000   1792  \n",
      "1793  0.125000   1793  \n",
      "1794  0.125000   1794  \n",
      "\n",
      "[1795 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = np.array(df)\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "kf.get_n_splits(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e0aabf5e1704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(train_inds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "#train_inds, test_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(dft, groups=dfo['modelID']))\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.2, random_state=0)\n",
    "train_inds, test_inds = next(gs.split(dft, labels, groups=dft['modelID']))\n",
    "\n",
    "\n",
    "train = df.iloc[train_inds]\n",
    "test = df.iloc[test_inds]\n",
    "#print(train_inds)\n",
    "#test_inds=test_inds[4::5]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([   5,    6,    7,    8,    9,   10,   11,   12,   13,   14,\\n            ...\\n            1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779],\\n           dtype='int64', length=1435)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-17d1fc52b1b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#rf.fit(train, labels[train_inds]);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#predictions = rf.predict(test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([   5,    6,    7,    8,    9,   10,   11,   12,   13,   14,\\n            ...\\n            1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779],\\n           dtype='int64', length=1435)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 200, random_state = 42)\n",
    "print(kf)\n",
    "\n",
    "train, test = dft[train_inds], dft[test_inds]\n",
    "#rf.fit(train, labels[train_inds]);\n",
    "#predictions = rf.predict(test)\n",
    "#Pred_TM =[predictions, labels[test_inds]]\n",
    "\n",
    "for train_inds, test_inds in gss.split(X, y, dft):\n",
    "    train, test = dft[train_inds], dft[test_inds]\n",
    "    rf.fit(train, labels[train_inds]);\n",
    "    predictions = rf.predict(test)\n",
    "    errors = abs(predictions - labels[test_inds])\n",
    "    #print('Mean Absolute Error:', round(np.mean(errors), 5), 'TM point.')\n",
    "    mape = 100 * (errors / labels[test_inds])\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    #MAE = mean_squared_error(labels[test_index], predictions)\n",
    "    #print('MAE',MAE,)\n",
    "    #print(round(accuracy, 2))  #print('Accuracy:', round(accuracy, 2), '%.')\n",
    "    \n",
    "    Pred_TM =[predictions, labels[test_inds]]\n",
    "    \n",
    "    corr, _ = pearsonr(predictions, labels[test_inds])\n",
    "    print( corr)\n",
    "    \n",
    "    #for v in zip(*Pred_TM):\n",
    "    #    print(*v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.63316229 -0.3246098   0.06657628 ... -1.11140442  3.36330234\n",
      "   1.53760099]\n",
      " [-0.14048346 -0.81686108  0.33936596 ...  0.4418695   0.63322442\n",
      "   1.07060605]\n",
      " [-1.5917397   0.07704488  1.25647325 ...  0.90744304 -1.13659003\n",
      "  -1.81836975]\n",
      " ...\n",
      " [ 1.63846369 -1.44735579 -0.46802685 ...  0.11457932 -1.18227305\n",
      "   0.72599442]\n",
      " [-1.79886344 -0.52941798 -0.19421666 ... -0.22486457 -0.0813801\n",
      "  -1.33117069]\n",
      " [ 1.05454024  0.47470609 -0.73670261 ... -0.86838853 -1.02166211\n",
      "   0.70167073]] [1 0 4 0 6 4 4 7 1 6 3 5 4 7 5 2 3 7 3 7 6 5 7 5 5 7 6 4 3 7 2 6 7 6 7 6 4\n",
      " 4 2 0 2 7 3 1 6 0 5 3 3 1 2 7 7 2 2 2 4 1 5 3 4 6 7 4 4 7 4 1 6 3 6 4 2 0\n",
      " 3 1 4 4 4 6 0 4 5 4 0 6 1 4 4 4 5 2 1 3 4 2 7 2 0 3 7 6 7 3 7 2 0 3 3 4 1\n",
      " 5 2 3 6 6 3 7 3 0 3 3 1 1 6 5 1 2 7 0 7 7 5 5 1 6 1 5 2 3 0 4 5 4 3 7 1 5\n",
      " 0 6 2 7 2 1 7 6 6 4 3 4 4 2 4 7 5 2 1 1 3 1 1 2 1 5 6 4 6 1 3 3 3 0 5 0 0\n",
      " 3 4 4 1 6 4 0 3 7 3 5 6 7 4 7 1 7 1 2 3 2 3 3 6 4 7 6 7 6 0 6 1 5 3 5 1 0\n",
      " 0 6 6 1 0 6 7 6 0 7 2 2 7 1 5 7 4 4 0 1 2 0 4 5 6 7 1 2 3 0 0 3 5 1 3 3 6\n",
      " 6 2 6 7 4 0 3 7 7 1 7 2 4 0 6 0 3 7 1 5 4 3 2 3 3 7 3 1 0 6 1 7 7 1 5 3 3\n",
      " 4 6 4 5 7 4 4 6 6 5 6 0 3 1 1 5 7 2 2 4 7 7 6 1 2 2 2 2 0 2 7 7 4 3 2 6 3\n",
      " 2 4 5 0 7 6 5 6 6 1 3 5 0 4 3 2 7 0 2 1 0 2 3 0 7 4 6 3 4 5 2 5 2 3 3 5 7\n",
      " 0 5 4 5 5 5 1 0 1 4 4 3 0 5 7 1 5 0 4 5 1 7 3 3 1 3 3 7 6 6 6 5 1 6 4 3 1\n",
      " 7 3 4 6 0 6 0 3 2 7 2 6 7 2 1 4 3 7 1 4 0 4 5 5 7 2 1 2 1 3 5 1 4 1 1 1 6\n",
      " 7 6 2 6 5 1 5 0 0 7 1 3 1 2 5 1 7 0 2 1 3 0 6 1 7 0 0 2 2 6 4 0 5 6 4 1 5\n",
      " 6 2 3 2 6 6 6 5 5 0 1 2 7 0 1 0 6 2 0 3 5 3 0 5 0 2 1 0 4 5 7 3 0 7 5 4 4\n",
      " 4 4 5 6 4 4 2 4 6 0 5 1 2 0 3 6 3 7 6 0 1 4 6 5 0 4 2 5 1 7 1 0 5 0 2 1 0\n",
      " 6 1 3 6 7 1 3 1 5 1 5 5 4 5 2 6 3 5 1 7 6 3 5 7 1 6 2 5 3 0 2 5 6 0 6 4 5\n",
      " 4 4 4 6 0 2 3 0 1 2 1 2 7 5 4 2 5 7 5 0 2 5 7 1 3 2 1 0 0 3 5 3 3 0 3 4 7\n",
      " 2 3 4 4 4 6 2 3 0 3 6 7 4 4 7 1 6 5 1 5 0 7 5 2 3 7 4 0 5 1 3 5 3 7 0 3 6\n",
      " 4 5 1 2 0 2 3 1 3 1 4 1 6 1 4 0 4 6 1 7 7 1 1 1 5 2 1 4 3 1 7 1 4 2 0 7 3\n",
      " 5 0 5 4 3 6 3 0 0 5 3 2 6 2 5 7 3 4 3 0 0 4 7 6 7 5 4 1 0 5 0 4 2 5 2 3 5\n",
      " 1 7 7 2 1 2 5 6 6 7 1 5 7 7 6 6 1 2 1 1 5 7 4 3 2 1 1 6 6 6 2 7 7 1 4 3 1\n",
      " 0 1 1 0 2 6 7 7 0 5 2 2 6 2 5 6 5 0 7 0 7 6 3 6 0 4 5 4 7 1 6 2 6 0 2 1 4\n",
      " 0 0 2 0 6 6 4 7 6 7 0 2 2 0 2 5 3 7 7 6 1 5 2 6 3 7 0 4 5 3 3 2 6 7 1 2 1\n",
      " 7 6 3 3 1 6 6 0 4 3 6 3 0 3 3 7 4 0 6 5 2 5 2 1 5 0 4 3 1 2 4 2 7 1 0 6 5\n",
      " 3 0 5 4 1 7 5 6 6 7 2 3 0 7 5 4 0 4 7 3 3 2 4 4 5 4 7 0 3 7 0 5 5 0 0 3 6\n",
      " 2 3 5 4 5 5 5 0 1 3 2 2 2 5 6 5 0 2 3 3 4 0 0 5 2 7 6 7 0 2 0 2 1 4 6 0 6\n",
      " 5 0 5 2 2 1 2 4 2 0 5 0 4 4 2 7 4 5 1 3 5 2 6 2 0 7 6 4 0 2 7 6 2 4 2 4 6\n",
      " 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2516f4355e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m               min_features_to_select=min_features_to_select)\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrfecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/yellowbrick/model_selection/rfecv.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn_features_to_select\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_feature_subsets_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features_to_select\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_features_to_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcv_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Convert scores to array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BaseScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 score = scorer._score(cached_call, estimator,\n\u001b[0;32m---> 87\u001b[0;31m                                       *args, **kwargs)\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[0;32m--> 212\u001b[0;31m                                                  **self._kwargs)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a classification task using 3 informative features\n",
    "X, y = make_classification(n_samples=1000, n_features=29, n_informative=3,\n",
    "                           n_redundant=2, n_repeated=0, n_classes=8,\n",
    "                           n_clusters_per_class=1, random_state=0)\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = RandomForestRegressor(n_estimators = 200, random_state = 42)\n",
    "# The \"accuracy\" scoring is proportional to the number of correct\n",
    "# classifications\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy',\n",
    "              min_features_to_select=min_features_to_select)\n",
    "print(X, y) \n",
    "rfecv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-eb270f5c1608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n\u001b[1;32m      3\u001b[0m              axis=0)\n\u001b[1;32m      4\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfeature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         all_importances = Parallel(n_jobs=self.n_jobs,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "feature_list = [\"pcons\",\"proq4\",\"dan\",\"seqlength\",\"Neff\",\"DIS\",\"FC\",\"FE\",\"FH\",\"A\",\"C\",\"E\",\"D\",\"G\",\"F\",\"I\",\"H\",\"K\",\"M\",\"L\",\"N\",\"Q\",\"P\",\"S\",\"R\",\"T\",\"W\",\"V\",\"Y\"] #names of features.\n",
    "ff = np.array(feature_list)\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(dft.shape[1]):\n",
    "        print(\"%d. feature: %s (%f)\" % (f + 1, ff[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dff =  pd.read_csv(\"features.csv\")\n",
    "\n",
    "\n",
    "ax = sns.barplot(x=\"Feature\", y=\"Importance\", data=dff)\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Features\")\n",
    "fig = ax.get_figure()\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Prediction   TMscore\n",
      "Prediction    1.000000  0.809861\n",
      "TMscore       0.809861  1.000000\n",
      "            Prediction   Tmscore\n",
      "Prediction    1.000000  0.981954\n",
      "Tmscore       0.981954  1.000000\n",
      "            Prediction   TMscore\n",
      "Prediction    1.000000  0.916765\n",
      "TMscore       0.916765  1.000000\n",
      "            Prediction  TMscore\n",
      "Prediction     1.00000  0.72365\n",
      "TMscore        0.72365  1.00000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dfp =  pd.read_csv(\"prediction.csv\")\n",
    "dfp2 =  pd.read_csv(\"prediction_2.csv\")\n",
    "dfpnodan =  pd.read_csv(\"prediction_nodan.csv\")\n",
    "dfpnodanlen =  pd.read_csv(\"prediction_nodanlen.csv\")\n",
    "\n",
    "\n",
    "Cor = dfp.corr(method='pearson')\n",
    "Cor2 = dfp2.corr(method='pearson')\n",
    "\n",
    "Cornodan = dfpnodan.corr(method='pearson')\n",
    "Cornodanlen = dfpnodanlen.corr(method='pearson')\n",
    "\n",
    "print(Cor)\n",
    "print(Cor2)\n",
    "print(Cornodan)\n",
    "print(Cornodanlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03935478945907345\n",
      "0.053617467317139955\n",
      "0.041446939021404\n",
      "0.06958280842330501\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dfp =  pd.read_csv(\"prediction_comparison.csv\")\n",
    "dfsd_nodan =  pd.read_csv(\"prediction_comparison_noDAN.csv\")\n",
    "\n",
    "Cor = dfp.corr(method='pearson')\n",
    "dfsd=dfp.groupby(['modelID'])['Prediction','Prediction_length'].std()\n",
    "print(dfsd['Prediction_length'].mean())\n",
    "print(dfsd['Prediction'].mean())\n",
    "\n",
    "dfsd_nodan2=dfsd_nodan.groupby(['modelID'])['Prediction_nodan','Prediction_nodanlen'].std()\n",
    "print(dfsd_nodan2['Prediction_nodan'].mean())\n",
    "print(dfsd_nodan2['Prediction_nodanlen'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dfr =  pd.read_csv(\"result.csv\")\n",
    "\n",
    "ax = sns.barplot(x=\"Predictor\", y=\"Pearson\", data=dfr)\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "ax.set_ylabel(\"Pearson correlation\")\n",
    "fig = ax.get_figure()\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
